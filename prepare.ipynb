{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcaf734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max text length 286\n",
      "max display text length 270\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_json('data/dataset_1.json')\n",
    "\n",
    "data[\"label_bin\"] = np.where(data[\"label\"] > 1, 1, 0)\n",
    "data[\"label_n\"] = data[\"label\"] - 1\n",
    "data[\"display_text\"] = [d[1]['text'][d[1]['displayTextRangeStart']: d[1]['getDisplayTextRangeEnd']] for d in data[[\"text\",\"displayTextRangeStart\", \"getDisplayTextRangeEnd\"]].iterrows()]\n",
    "print(\"max text length\", len(data.iloc[np.argmax(data['text'].to_numpy())]['text']))\n",
    "max_display_text_length = len(data.iloc[np.argmax(data['display_text'].to_numpy())]['display_text'])\n",
    "print(\"max display text length\", max_display_text_length)\n",
    "data\n",
    "X = data.display_text.to_list()\n",
    "y = data.label_n.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95af0ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    \n",
    "# ----- 2. Fine-tune pretrained model -----#\n",
    "# Define Trainer parameters\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    \n",
    "def train_model(X, y, validation_split_ratio=0.2, batch_size=16, model_name=\"bert-base-uncased\", random_seed=1337):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=validation_split_ratio, random_state=random_seed, shuffle=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_split_ratio * len(X) / len(X_train), random_state=random_seed, shuffle=True)\n",
    "\n",
    "    X_train_trans = tokenizer(X, truncation=True, padding=True, max_length=512)\n",
    "    X_test_trans = tokenizer(X_test, truncation=True, padding=True, max_length=512)\n",
    "    X_val_trans = tokenizer(X_val, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "    X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "\n",
    "    # Define Trainer\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"output\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=1,\n",
    "        seed=0,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "        \n",
    "    train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "    val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    )\n",
    "\n",
    "    # Train pre-trained model\n",
    "    trainer.train()\n",
    "\n",
    "    # ----- 3. Predict -----#\n",
    "    # Load test data\n",
    "    # test_data = pd.read_csv(\"test.csv\")\n",
    "    # X_test = list(test_data[\"review\"])\n",
    "    X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Create torch dataset\n",
    "    test_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "    # # Load trained model\n",
    "    # model_path = \"./output/checkpoint-50000\"\n",
    "    # model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "    # Make prediction\n",
    "    raw_pred, _, _ = trainer.predict(test_dataset)\n",
    "\n",
    "    # Preprocess raw predictions\n",
    "    y_pred = np.argmax(raw_pred, axis=1)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21b9fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.3.output.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.embeddings.LayerNorm.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'cls.predictions.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.6.attention.k_lin.weight', 'pre_classifier.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'classifier.weight', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.10.attention.v_lin.bias', 'embeddings.LayerNorm.bias', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.4.attention.k_lin.bias', 'classifier.bias', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.10.ffn.lin2.weight', 'embeddings.position_embeddings.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.0.attention.q_lin.weight', 'pre_classifier.bias', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.weight', 'embeddings.LayerNorm.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.2.attention.k_lin.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3915\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 245\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [245/245 01:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1305\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x7f6e7bd87490>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f051bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sbert_centroid_args(sentences, num_labels:int, bert_model_name='all-distilroberta-v1', verbose=False):\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    import torch\n",
    "    model = SentenceTransformer(bert_model_name)\n",
    "    model.max_seq_length = np.argmax(sentences)\n",
    "    \n",
    "    embedding_list = model.encode(sentences, show_progress_bar=verbose)\n",
    "    from sklearn.cluster import KMeans\n",
    "    clustering_model = KMeans(n_clusters=num_labels, random_state=1337) \n",
    "    clustering_model.fit(embedding_list)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    clustered_sentences = {}\n",
    "    for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "        if cluster_id not in clustered_sentences:\n",
    "            clustered_sentences[cluster_id] = []\n",
    "\n",
    "        clustered_sentences[cluster_id].append(sentence_id)\n",
    "\n",
    "    centroids = []\n",
    "    for i in range(len(clustering_model.cluster_centers_)):\n",
    "        center = clustering_model.cluster_centers_[i]\n",
    "        # get centroid arg for cluster by min euclidian distance from cluster center\n",
    "        centroid_arg = clustered_sentences[i][np.argmin([np.linalg.norm(embedding_list[cluster_item_arg]-center) for cluster_item_arg in clustered_sentences[i]])]\n",
    "        centroids.append(centroid_arg)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69c8ada6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[90,\n",
       " 184,\n",
       " 355,\n",
       " 384,\n",
       " 425,\n",
       " 503,\n",
       " 1197,\n",
       " 1557,\n",
       " 2011,\n",
       " 2539,\n",
       " 2732,\n",
       " 2902,\n",
       " 3012,\n",
       " 3050,\n",
       " 3237,\n",
       " 3253,\n",
       " 3328,\n",
       " 3411,\n",
       " 3523,\n",
       " 3703,\n",
       " 4064,\n",
       " 4117,\n",
       " 4249,\n",
       " 4325,\n",
       " 4629,\n",
       " 5030,\n",
       " 5397,\n",
       " 5749,\n",
       " 5850,\n",
       " 6173]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_args = get_sbert_centroid_args(sentences=X, num_labels=30)\n",
    "centroid_args.sort()\n",
    "centroid_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
