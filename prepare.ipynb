{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcaf734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "random_seed = 1337\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a59d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get and prepare training data\n",
    "def get_training_data(dataset_path:str, test_split_ratio:float=0.2,verbose=False):\n",
    "    data = pd.read_json(dataset_path)\n",
    "    data[\"label_train\"] = data[\"label\"] - 1\n",
    "    data[\"display_text\"] = [d[1]['text'][d[1]['displayTextRangeStart']: d[1]['getDisplayTextRangeEnd']] for d in data[[\"text\",\"displayTextRangeStart\", \"getDisplayTextRangeEnd\"]].iterrows()]\n",
    "    if verbose : print(\"max text length\", len(data.iloc[np.argmax(data['text'].to_numpy())]['text']))\n",
    "    max_display_text_length = len(data.iloc[np.argmax(data['display_text'].to_numpy())]['display_text'])\n",
    "    if verbose : print(\"max display text length\", max_display_text_length)\n",
    "    X = data.display_text.to_list()\n",
    "    y = data.label_train.to_list()\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split_ratio, random_state=random_seed, shuffle=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_split_ratio * len(X) / len(X_train), random_state=random_seed, shuffle=True)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95af0ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred, labels):\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=100, epochs=3, model=None, model_name = 'distilbert-base-uncased'):\n",
    "    \n",
    "    # BEGIN disable logging \n",
    "    import logging\n",
    "    def set_global_logging_level(level=logging.ERROR, prefices=[\"\"]):\n",
    "        import re\n",
    "        prefix_re = re.compile(fr'^(?:{ \"|\".join(prefices) })')\n",
    "        for name in logging.root.manager.loggerDict:\n",
    "            if re.match(prefix_re, name):\n",
    "                logging.getLogger(name).setLevel(level)\n",
    "    set_global_logging_level(logging.CRITICAL) # disable INFO and DEBUG logging everywhere\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # END disable logging\n",
    "    \n",
    "    # BEGIN Set determinism !! must be inside function in every loop to work\n",
    "\n",
    "    from os import environ\n",
    "    environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    # !! important !! import torch after setting cublas deterministic or it will not work !!\n",
    "    import torch\n",
    "    from transformers import TrainingArguments, Trainer, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "    import transformers\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    import random\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # END Set determinism\n",
    "    \n",
    "     # Create torch dataset\n",
    "    class Dataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels=None):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            if self.labels: item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "        def __len__(self):\n",
    "            return len(self.encodings[\"input_ids\"])\n",
    "    \n",
    "    #disable logging\n",
    "    #transformers.logging.set_verbosity(transformers.logging.CRITICAL)\n",
    "    \n",
    "    # create tokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name) \n",
    "    \n",
    "    # create datasets\n",
    "    train_dataset = Dataset(tokenizer(X_train, truncation=True, padding=True, max_length=512), y_train)\n",
    "    val_dataset = Dataset(tokenizer(X_val, truncation=True, padding=True, max_length=512), y_val)\n",
    "    test_dataset = Dataset(tokenizer(X_test, padding=True, truncation=True, max_length=512), y_test)\n",
    "    \n",
    "    #create model\n",
    "\n",
    "    if model is None:\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "\n",
    "\n",
    "    #training settings\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"output\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        eval_steps=1,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        seed=random_seed,\n",
    "        load_best_model_at_end=False\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=lambda p: compute_metrics(p[0], p[1])\n",
    "    )\n",
    "    # disable print log\n",
    "    from transformers.trainer_callback import PrinterCallback\n",
    "    trainer.remove_callback(PrinterCallback)\n",
    "\n",
    "    # Train\n",
    "    trainer.train()\n",
    "\n",
    "    # Test\n",
    "    metrics = trainer.evaluate(test_dataset, metric_key_prefix=\"\")\n",
    "\n",
    "#     raw_pred, _, _ = trainer.predict(test_dataset)\n",
    "#     m = compute_metrics(raw_pred, y_test)\n",
    "    return metrics, trainer, trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21b9fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 3915\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 100\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 100\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.435033</td>\n",
       "      <td>0.826820</td>\n",
       "      <td>0.797222</td>\n",
       "      <td>0.826820</td>\n",
       "      <td>0.780600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.345293</td>\n",
       "      <td>0.884291</td>\n",
       "      <td>0.894065</td>\n",
       "      <td>0.884291</td>\n",
       "      <td>0.866917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.329996</td>\n",
       "      <td>0.885824</td>\n",
       "      <td>0.882407</td>\n",
       "      <td>0.885824</td>\n",
       "      <td>0.873830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1305\n",
      "  Batch size = 100\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1305\n",
      "  Batch size = 100\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1305\n",
      "  Batch size = 100\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1305\n",
      "  Batch size = 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "({'_loss': 0.34015172719955444,\n",
       "  '_accuracy': 0.871264367816092,\n",
       "  '_precision': 0.8601053715194783,\n",
       "  '_recall': 0.871264367816092,\n",
       "  '_f1': 0.8568178278784206,\n",
       "  '_runtime': 0.9792,\n",
       "  '_samples_per_second': 1332.783,\n",
       "  '_steps_per_second': 14.298,\n",
       "  'epoch': 3.0},\n",
       " {'_loss': 0.34015172719955444,\n",
       "  '_accuracy': 0.871264367816092,\n",
       "  '_precision': 0.8601053715194783,\n",
       "  '_recall': 0.871264367816092,\n",
       "  '_f1': 0.8568178278784206,\n",
       "  '_runtime': 0.9624,\n",
       "  '_samples_per_second': 1355.936,\n",
       "  '_steps_per_second': 14.546,\n",
       "  'epoch': 3.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = get_training_data('data/dataset_1.json')\n",
    "metrics1, trainer1, model1 = train_model(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "metrics2, trainer2, model2 = train_model(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "metrics1, metrics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20bb696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbert_tokenize(sentences, verbose=False, bert_model_name='all-distilroberta-v1'):\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    import torch\n",
    "    model = SentenceTransformer(bert_model_name)\n",
    "    model.max_seq_length = np.argmax(sentences)\n",
    "    \n",
    "    embedding_list = model.encode(sentences, show_progress_bar=verbose)\n",
    "    return embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e16d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans = sbert_tokenize(X_train)\n",
    "X_val_trans = sbert_tokenize(X_val)\n",
    "X_test_trans = sbert_tokenize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f051bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sbert_centroid_args(sentences, num_labels:int, bert_model_name='all-distilroberta-v1', verbose=False):\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    import torch\n",
    "    model = SentenceTransformer(bert_model_name)\n",
    "    model.max_seq_length = np.argmax(sentences)\n",
    "    \n",
    "    embedding_list = model.encode(sentences, show_progress_bar=verbose)\n",
    "    from sklearn.cluster import KMeans\n",
    "    clustering_model = KMeans(n_clusters=num_labels, random_state=1337) \n",
    "    clustering_model.fit(embedding_list)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    clustered_sentences = {}\n",
    "    for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "        if cluster_id not in clustered_sentences:\n",
    "            clustered_sentences[cluster_id] = []\n",
    "\n",
    "        clustered_sentences[cluster_id].append(sentence_id)\n",
    "\n",
    "    centroids = []\n",
    "    for i in range(len(clustering_model.cluster_centers_)):\n",
    "        center = clustering_model.cluster_centers_[i]\n",
    "        # get centroid arg for cluster by min euclidian distance from cluster center\n",
    "        centroid_arg = clustered_sentences[i][np.argmin([np.linalg.norm(embedding_list[cluster_item_arg]-center) for cluster_item_arg in clustered_sentences[i]])]\n",
    "        centroids.append(centroid_arg)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c8ada6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[163,\n",
       " 342,\n",
       " 369,\n",
       " 381,\n",
       " 439,\n",
       " 493,\n",
       " 496,\n",
       " 540,\n",
       " 666,\n",
       " 707,\n",
       " 851,\n",
       " 1207,\n",
       " 1313,\n",
       " 1332,\n",
       " 1401,\n",
       " 1711,\n",
       " 1982,\n",
       " 2198,\n",
       " 2238,\n",
       " 2551,\n",
       " 2605,\n",
       " 2654,\n",
       " 2697,\n",
       " 3031,\n",
       " 3122,\n",
       " 3173,\n",
       " 3520,\n",
       " 3583,\n",
       " 3701,\n",
       " 3863]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_args = get_sbert_centroid_args(sentences=X_train, num_labels=30)\n",
    "centroid_args.sort()\n",
    "centroid_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e596aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sampling_args(embedding_list, num_labels:int, verbose=False):\n",
    "    import random\n",
    "    random.seed(1337)\n",
    "    return random.sample(range(0, len(embedding_list)), num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bd9bdc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[447,\n",
       " 678,\n",
       " 839,\n",
       " 1260,\n",
       " 1363,\n",
       " 1473,\n",
       " 1494,\n",
       " 1498,\n",
       " 1571,\n",
       " 1603,\n",
       " 1742,\n",
       " 2184,\n",
       " 2338,\n",
       " 2399,\n",
       " 2530,\n",
       " 2600,\n",
       " 2601,\n",
       " 2688,\n",
       " 2855,\n",
       " 2907,\n",
       " 2997,\n",
       " 3119,\n",
       " 3175,\n",
       " 3234,\n",
       " 3360,\n",
       " 3376,\n",
       " 3538,\n",
       " 3736,\n",
       " 3775,\n",
       " 3790]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_args = get_random_sampling_args(embedding_list=X_train_trans, num_labels=30)\n",
    "random_args.sort()\n",
    "random_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ade4bb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'_loss': 1.2663761377334595,\n",
       "   '_accuracy': 0.7578544061302682,\n",
       "   '_precision': 0.6121361436434396,\n",
       "   '_recall': 0.7578544061302682,\n",
       "   '_f1': 0.6772456549987139,\n",
       "   '_runtime': 0.9463,\n",
       "   '_samples_per_second': 1379.052,\n",
       "   '_steps_per_second': 14.794,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 100},\n",
       "  {'_loss': 1.1148953437805176,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.955,\n",
       "   '_samples_per_second': 1366.522,\n",
       "   '_steps_per_second': 14.66,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 200},\n",
       "  {'_loss': 0.9653891921043396,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.9484,\n",
       "   '_samples_per_second': 1375.971,\n",
       "   '_steps_per_second': 14.761,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 300},\n",
       "  {'_loss': 0.8574875593185425,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.9476,\n",
       "   '_samples_per_second': 1377.132,\n",
       "   '_steps_per_second': 14.774,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 400},\n",
       "  {'_loss': 0.8020381331443787,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.9481,\n",
       "   '_samples_per_second': 1376.383,\n",
       "   '_steps_per_second': 14.766,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 500},\n",
       "  {'_loss': 0.7560577392578125,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.9484,\n",
       "   '_samples_per_second': 1376.021,\n",
       "   '_steps_per_second': 14.762,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 600},\n",
       "  {'_loss': 0.7193027138710022,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.9485,\n",
       "   '_samples_per_second': 1375.853,\n",
       "   '_steps_per_second': 14.76,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 700},\n",
       "  {'_loss': 0.6844804883003235,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.948,\n",
       "   '_samples_per_second': 1376.653,\n",
       "   '_steps_per_second': 14.769,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 800},\n",
       "  {'_loss': 0.6556736826896667,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.948,\n",
       "   '_samples_per_second': 1376.639,\n",
       "   '_steps_per_second': 14.769,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 900},\n",
       "  {'_loss': 0.6776504516601562,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.9528,\n",
       "   '_samples_per_second': 1369.597,\n",
       "   '_steps_per_second': 14.693,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 1000}],\n",
       " [{'_loss': 1.2749381065368652,\n",
       "   '_accuracy': 0.753256704980843,\n",
       "   '_precision': 0.6113128445014758,\n",
       "   '_recall': 0.753256704980843,\n",
       "   '_f1': 0.6749022050746187,\n",
       "   '_runtime': 0.9475,\n",
       "   '_samples_per_second': 1377.292,\n",
       "   '_steps_per_second': 14.776,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 100},\n",
       "  {'_loss': 1.1266603469848633,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.9497,\n",
       "   '_samples_per_second': 1374.078,\n",
       "   '_steps_per_second': 14.741,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 200},\n",
       "  {'_loss': 0.9771448969841003,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.945,\n",
       "   '_samples_per_second': 1380.942,\n",
       "   '_steps_per_second': 14.815,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 300},\n",
       "  {'_loss': 0.8613560199737549,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.9491,\n",
       "   '_samples_per_second': 1374.916,\n",
       "   '_steps_per_second': 14.75,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 400},\n",
       "  {'_loss': 0.791322648525238,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.9461,\n",
       "   '_samples_per_second': 1379.325,\n",
       "   '_steps_per_second': 14.797,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 500},\n",
       "  {'_loss': 0.7475742101669312,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.9469,\n",
       "   '_samples_per_second': 1378.125,\n",
       "   '_steps_per_second': 14.784,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 600},\n",
       "  {'_loss': 0.7172151803970337,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.9497,\n",
       "   '_samples_per_second': 1374.168,\n",
       "   '_steps_per_second': 14.742,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 700},\n",
       "  {'_loss': 0.7058423757553101,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.9487,\n",
       "   '_samples_per_second': 1375.603,\n",
       "   '_steps_per_second': 14.757,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 800},\n",
       "  {'_loss': 0.6788997054100037,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.948,\n",
       "   '_samples_per_second': 1376.597,\n",
       "   '_steps_per_second': 14.768,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 900},\n",
       "  {'_loss': 0.6681555509567261,\n",
       "   '_accuracy': 0.7854406130268199,\n",
       "   '_precision': 0.6169169565919467,\n",
       "   '_recall': 0.7854406130268199,\n",
       "   '_f1': 0.6910528998733824,\n",
       "   '_runtime': 0.952,\n",
       "   '_samples_per_second': 1370.845,\n",
       "   '_steps_per_second': 14.706,\n",
       "   'epoch': 1.0,\n",
       "   'trained_samples': 1000}])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_active_learning(algorithm, source, source_y, batch_size=100):\n",
    "    res = []\n",
    "    source = list(source)\n",
    "    source_y = list(source_y)\n",
    "    model = None\n",
    "    i = 0\n",
    "    for j in range(10):\n",
    "        pick_args = algorithm(source, batch_size)\n",
    "        #sort reverse or pop will end with argument out of range exception\n",
    "        pick_args.sort(reverse=True)\n",
    "        samples = []\n",
    "        samples_y = []\n",
    "        # transfer samples from embedding list to samples\n",
    "        for d in pick_args: \n",
    "            samples.append(source.pop(d))\n",
    "            samples_y.append(source_y.pop(d))\n",
    "        \n",
    "        metric, trainer, model_t = train_model(samples, samples_y, X_val, y_val, X_test, y_test, batch_size=100, epochs=1, model=model)\n",
    "        model = model_t\n",
    "        i = i + len(samples)\n",
    "        metric[\"trained_samples\"] = i\n",
    "        res.append(metric)\n",
    "    return res\n",
    "res_rand = apply_active_learning(get_random_sampling_args, X_train, y_train)\n",
    "res_centr = apply_active_learning(get_sbert_centroid_args, X_train, y_train)\n",
    "res_rand, res_centr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
