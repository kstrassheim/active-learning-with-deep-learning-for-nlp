{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcaf734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "random_seed = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a59d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get and prepare training data\n",
    "def get_training_data(dataset_path:str, test_split_ratio:float=0.2,verbose=False):\n",
    "    data = pd.read_json(dataset_path)\n",
    "    data[\"label_train\"] = data[\"label\"] - 1\n",
    "    data[\"display_text\"] = [d[1]['text'][d[1]['displayTextRangeStart']: d[1]['getDisplayTextRangeEnd']] for d in data[[\"text\",\"displayTextRangeStart\", \"getDisplayTextRangeEnd\"]].iterrows()]\n",
    "    if verbose : print(\"max text length\", len(data.iloc[np.argmax(data['text'].to_numpy())]['text']))\n",
    "    max_display_text_length = len(data.iloc[np.argmax(data['display_text'].to_numpy())]['display_text'])\n",
    "    if verbose : print(\"max display text length\", max_display_text_length)\n",
    "    X = data.display_text.to_list()\n",
    "    y = data.label_train.to_list()\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split_ratio, random_state=random_seed, shuffle=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_split_ratio * len(X) / len(X_train), random_state=random_seed, shuffle=True)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95af0ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred, labels):\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                validation_split_ratio=0.2, batch_size=100, epochs=3, model_name='distilbert-base-uncased'):\n",
    "    \n",
    "    # BEGIN disable logging \n",
    "    import logging\n",
    "    def set_global_logging_level(level=logging.ERROR, prefices=[\"\"]):\n",
    "        import re\n",
    "        prefix_re = re.compile(fr'^(?:{ \"|\".join(prefices) })')\n",
    "        for name in logging.root.manager.loggerDict:\n",
    "            if re.match(prefix_re, name):\n",
    "                logging.getLogger(name).setLevel(level)\n",
    "    set_global_logging_level(logging.CRITICAL) # disable INFO and DEBUG logging everywhere\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # END disable logging\n",
    "    \n",
    "    # BEGIN Set determinism !! must be inside function in every loop to work\n",
    "\n",
    "    from os import environ\n",
    "    environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    # !! important !! import torch after setting cublas deterministic or it will not work !!\n",
    "    import torch\n",
    "    from transformers import TrainingArguments, Trainer, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "    import transformers\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    import random\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # END Set determinism\n",
    "    \n",
    "     # Create torch dataset\n",
    "    class Dataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels=None):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            if self.labels: item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "        def __len__(self):\n",
    "            return len(self.encodings[\"input_ids\"])\n",
    "    \n",
    "    #disable logging\n",
    "    #transformers.logging.set_verbosity(transformers.logging.CRITICAL)\n",
    "    \n",
    "    \n",
    "    #create model\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name) \n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "    \n",
    "    # create datasets\n",
    "    train_dataset = Dataset(tokenizer(X_train, truncation=True, padding=True, max_length=512), y_train)\n",
    "    val_dataset = Dataset(tokenizer(X_val, truncation=True, padding=True, max_length=512), y_val)\n",
    "    test_dataset = Dataset(tokenizer(X_test, padding=True, truncation=True, max_length=512), y_test)\n",
    "    \n",
    "    #training settings\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"output\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        eval_steps=1,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        seed=random_seed,\n",
    "        load_best_model_at_end=False\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=lambda p: compute_metrics(p[0], p[1])\n",
    "    )\n",
    "    # disable print log\n",
    "    from transformers.trainer_callback import PrinterCallback\n",
    "    trainer.remove_callback(PrinterCallback)\n",
    "\n",
    "    # Train\n",
    "    trainer.train()\n",
    "\n",
    "    # Test\n",
    "    metrics = trainer.evaluate(test_dataset, metric_key_prefix=\"\")\n",
    "\n",
    "#     raw_pred, _, _ = trainer.predict(test_dataset)\n",
    "#     m = compute_metrics(raw_pred, y_test)\n",
    "    return metrics, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b9fd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'_loss': 0.34015172719955444,\n",
       "  '_accuracy': 0.871264367816092,\n",
       "  '_precision': 0.8601053715194783,\n",
       "  '_recall': 0.871264367816092,\n",
       "  '_f1': 0.8568178278784206,\n",
       "  '_runtime': 0.9638,\n",
       "  '_samples_per_second': 1353.962,\n",
       "  '_steps_per_second': 14.525,\n",
       "  'epoch': 3.0},\n",
       " {'_loss': 0.34015172719955444,\n",
       "  '_accuracy': 0.871264367816092,\n",
       "  '_precision': 0.8601053715194783,\n",
       "  '_recall': 0.871264367816092,\n",
       "  '_f1': 0.8568178278784206,\n",
       "  '_runtime': 0.9646,\n",
       "  '_samples_per_second': 1352.825,\n",
       "  '_steps_per_second': 14.513,\n",
       "  'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = get_training_data('data/dataset_1.json')\n",
    "metrics1, trainer1 = train_model(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "metrics2, trainer2 = train_model(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "metrics1, metrics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f051bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sbert_centroid_args(sentences, num_labels:int, bert_model_name='all-distilroberta-v1', verbose=False):\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    import torch\n",
    "    model = SentenceTransformer(bert_model_name)\n",
    "    model.max_seq_length = np.argmax(sentences)\n",
    "    \n",
    "    embedding_list = model.encode(sentences, show_progress_bar=verbose)\n",
    "    from sklearn.cluster import KMeans\n",
    "    clustering_model = KMeans(n_clusters=num_labels, random_state=1337) \n",
    "    clustering_model.fit(embedding_list)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    clustered_sentences = {}\n",
    "    for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "        if cluster_id not in clustered_sentences:\n",
    "            clustered_sentences[cluster_id] = []\n",
    "\n",
    "        clustered_sentences[cluster_id].append(sentence_id)\n",
    "\n",
    "    centroids = []\n",
    "    for i in range(len(clustering_model.cluster_centers_)):\n",
    "        center = clustering_model.cluster_centers_[i]\n",
    "        # get centroid arg for cluster by min euclidian distance from cluster center\n",
    "        centroid_arg = clustered_sentences[i][np.argmin([np.linalg.norm(embedding_list[cluster_item_arg]-center) for cluster_item_arg in clustered_sentences[i]])]\n",
    "        centroids.append(centroid_arg)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69c8ada6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[163,\n",
       " 342,\n",
       " 369,\n",
       " 381,\n",
       " 439,\n",
       " 493,\n",
       " 496,\n",
       " 540,\n",
       " 666,\n",
       " 707,\n",
       " 851,\n",
       " 1207,\n",
       " 1313,\n",
       " 1332,\n",
       " 1401,\n",
       " 1711,\n",
       " 1982,\n",
       " 2198,\n",
       " 2238,\n",
       " 2551,\n",
       " 2605,\n",
       " 2654,\n",
       " 2697,\n",
       " 3031,\n",
       " 3122,\n",
       " 3173,\n",
       " 3520,\n",
       " 3583,\n",
       " 3701,\n",
       " 3863]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_args = get_sbert_centroid_args(sentences=X_train, num_labels=30)\n",
    "centroid_args.sort()\n",
    "centroid_args"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
